
Expr_Id :: enum u8 {
    EXPR_IDENTIFIER     = 0; // no struct
    EXPR_STRING         = 1;
    EXPR_NUMBER         = 2;
    EXPR_LITERAL        = 3;
    EXPR_UNARY          = 4;
    EXPR_BINARY         = 5;
    EXPR_PROCEDURE_TYPE = 6;
    EXPR_PROCEDURE_CALL = 7;
    EXPR_PROCEDURE      = 8;
    EXPR_CODE           = 9;
    EXPR_IMPORT         = 10; // no struct
    EXPR_STRUCT         = 11;
    EXPR_ENUM           = 12;
    EXPR_ARRAY          = 13;
    EXPR_SPECIAL        = 14;
    STMT_RETURN         = 15;
    STMT_DECLARATION    = 16;
    STMT_ASSIGNMENT     = 17;
    STMT_IF             = 18;
    STMT_WHILE          = 19;
    STMT_FOR            = 20;
    STMT_CONTROL        = 21;
    STMT_COMPOUND       = 22;
}

Special_Kind :: enum u8 {
    SPECIAL_EVAL_TYPE   = 0; // #type
    SPECIAL_EVAL_SIZE   = 1; // #size
    SPECIAL_TYPE        = 2; // #Type
    SPECIAL_NUMBER      = 3; // #Number
    SPECIAL_CODE        = 4; // #Code
}

Control_Kind :: enum u8 {
    CONTROL_CASE     = 0;
    CONTROL_BREAK    = 1;
    CONTROL_CONTINUE = 2;
    CONTROL_THROUGH  = 3;
    CONTROL_DEFER    = 4;
}

Expr_Flags :: enum u8 {
    FLAG_DECL_CONST       = 1 << 0;
    FLAG_DECL_USING       = 1 << 1;
    FLAG_EXPR_USING       = 1 << 1;
    FLAG_PROC_USING       = 1 << 1;
    FLAG_DECL_EXPORT      = 1 << 2;
    FLAG_STRUCT_UNION     = 1 << 2;
    FLAG_IF_CASE          = 1 << 2;
    FLAG_FOR_LESS_THAN    = 1 << 2;
    FLAG_ARRAY_DYNAMIC    = 1 << 2;
    FLAG_DECL_HOST_IMPORT = 1 << 3;
}

Tag :: struct {
    name : string;
    expr : *Expr;
    next : *Tag;
}

Expr :: struct {
    id          : Expr_Id;
    flags       : Expr_Flags;
    source_code : string;
    name        : string;
    next        : *Expr;
    tag         : *Tag;
    this_type   : *Type_Info;
    line_number : u32;
}

Stmt :: Expr;

Expr_String :: struct {
    using Expr;
    value : string;
}

Expr_Number :: struct {
    using Expr;
    value : Number;
}

Expr_Literal :: struct {
    using Expr;
    head: *Expr;
    count: u32;
}

Expr_Unary :: struct {
    using Expr;
    expr : *Expr;
    op : u32;
}

Expr_Binary :: struct {
    using Expr;
    left : *Expr;
    right : *Expr;
    op : u32;
}

Expr_Procedure_Call :: struct {
    using Expr;
    proc : *Expr;
    arg_head : *Expr;
    arg_count : u8;
}
 
Expr_Procedure_Type :: struct {
    using Expr;
    in_out_expr : *Expr;
    in_count : u8;
    out_count : u8;
}

Expr_Procedure :: struct {
    using Expr;
    in_out_expr : *Expr;
    body : *Stmt;
    in_count : u8;
    out_count : u8;
}

Expr_Struct :: struct {
    using Expr;
    field_count : u32;
    head : *Stmt;
}

Expr_Enum :: struct {
    using Expr;
    type : *Expr;
    field_count : u32;
    head : *Stmt;
}

Expr_Array :: struct {
    using Expr;
    rows : *Expr;
    cols : *Expr;
    expr : *Expr;
}

Expr_Special :: struct {
    using Expr;
    kind : u8;
}

Stmt_Return :: struct {
    using Expr;
    expr : *Expr; // optional
}

Stmt_Declaration :: struct {
    using Expr;
//  more_names : [] string;
    value : *Expr;
    type  : *Expr; // optional
}

Stmt_Assignment :: struct {
    using Expr;
    op    : u32;
    dest  : *Expr;
    value : *Expr;
}

Stmt_Compound :: struct {
    using Expr;
    head : *Stmt;
}

Stmt_If :: struct {
    using Expr;
    condition : *Expr;
    then_body : *Stmt;
    else_body : *Stmt; @comment ("optional")
}

Stmt_While :: struct {
    using Expr;
    condition : *Expr;
    body : *Stmt;
}

Stmt_For :: struct {
    using Expr;
    body : *Stmt;
    from : *Expr;
    to   : *Expr; // optional (interates through `from` if this is null)
    iterator_name : string;
}

Stmt_Control :: struct {
    using Expr;
    kind : u8;
    expr : *Expr;
}

Syntax_Tree :: struct {
    root            : Stmt_Compound;
    source          : Source;
    allocator       : Arena_Allocator; // TODO: arena for each syntax tree? or each group of syntax trees?
}

Syntax_Tree_Create_Info :: struct {
    source     : Source;    // input
    allocator  : Allocator; // input
    error      : *Error;    // [output]
}

//Linked_List :: struct (T: #Type) {
//    head : *T;
//    last : *T;
//}

Expr_List :: LINKED_LIST(Expr);
Stmt_List :: LINKED_LIST(Stmt);

// Keywords

_keyword_map: [32] u8 = .{
    8, // if
    10, // ret
    1, // case
    14, // while
    4, // defer
    8, // if
    8, // if
    8, // if
    13, // using
    8, // if
    12, // union
    8, // if
    0, // break
    9, // loop
    8, // if
    8, // if
    3, // continue
    2, // cast
    8, // if
    6, // enum
    5, // else
    11, // struct
    7, // for
    8, // if
    8, // if
    8, // if
    8, // if
    8, // if
    8, // if
    8, // if
    8, // if
    8, // if
};

_keywords: [15] string = .{
    CONST_STRING("break"),
    CONST_STRING("case"),
    CONST_STRING("cast"),
    CONST_STRING("continue"),
    CONST_STRING("defer"),
    CONST_STRING("else"),
    CONST_STRING("enum"),
    CONST_STRING("for"),
    CONST_STRING("if"),
    CONST_STRING("loop"),
    CONST_STRING("ret"),
    CONST_STRING("struct"),
    CONST_STRING("union"),
    CONST_STRING("using"),
    CONST_STRING("while"),
};

_hash_keyword :: (s: string) -> u32 {
    h: u32 = 0;
    i: u32 = 0;
    while i < 8 && i < s.count {
        h = ((h << 4) + h) + s.data[i] + #char "a";
        i += 1;
    }
    ret h % 27;
}

// Tokenizer

Token_Id :: enum u32 {
    TOKEN_END        = 0;
    TOKEN_IDENTIFIER = 1;
    TOKEN_NUMBER     = 2;
    TOKEN_DIRECTIVE  = 3;
    TOKEN_TAG        = 4;
    TOKEN_STRING     = 5;
    // ASCII [33, 126] 
    TOKEN_break      = 0x80;
    TOKEN_case       = 0x81;
    TOKEN_cast       = 0x82;
    TOKEN_continue   = 0x83; // ? rename to skip
    TOKEN_defer      = 0x84;
    TOKEN_else       = 0x85;
    TOKEN_enum       = 0x86;
    TOKEN_for        = 0x87;
    TOKEN_if         = 0x88;
    TOKEN_loop       = 0x89;
    TOKEN_ret        = 0x8A;
    TOKEN_struct     = 0x8B;
    TOKEN_union      = 0x8C;
    TOKEN_using      = 0x8D;
    TOKEN_while      = 0x8E;
    // multi-characters [0xFF, ..]
}

Token :: struct {
    id : Token_Id;
    line_number : u32;
    string : string; // TODO: rename to `source_code`
    value : struct { // TODO: why struct and not union?
        string : string;
        number : Number;
    };
}

write_token :: (writer: *Writer, token: Token)
{
    symbol: bool = false;
    if token.id == {
        case KAI_TOKEN_IDENTIFIER;
            _write("Ident");
        case KAI_TOKEN_STRING;
            _write("String");
        case KAI_TOKEN_NUMBER;
            _write("Number");
        case KAI_TOKEN_DIRECTIVE;
            _write("Dir");
        case KAI_TOKEN_TAG;
            _write("Tag");
        case;
        symbol = true;
    }
    if !symbol _write("(");
    if token.id > 256 {
        id: u32 = token.id;
        ch: u8;
        str: string = string.{count = 1, data = *ch};

        while id != 0 {
            ch = id & 0xFF;
            _write_string(str);
            id = id >> 8;
        }
    }
    else
        _write_string(token.string);
    if !symbol _write(")");
}

token_string :: (id: Token_Id, dst: string) -> string
{
    assert(dst.count >= 12);

    if id == {
    case KAI_TOKEN_END; ret string_copy_from_c(dst, "end of file");
    case KAI_TOKEN_IDENTIFIER; ret string_copy_from_c(dst, "identifier");
    case KAI_TOKEN_NUMBER; ret string_copy_from_c(dst, "number");
    case KAI_TOKEN_DIRECTIVE; ret string_copy_from_c(dst, "directive");
    case KAI_TOKEN_TAG; ret string_copy_from_c(dst, "tag");
    case KAI_TOKEN_STRING; ret string_copy_from_c(dst, "string");
    case KAI_TOKEN_break; ret string_copy_from_c(dst, "'break'");
    case KAI_TOKEN_case; ret string_copy_from_c(dst, "'case'");
    case KAI_TOKEN_cast; ret string_copy_from_c(dst, "'cast'");
    case KAI_TOKEN_continue; ret string_copy_from_c(dst, "'continue'");
    case KAI_TOKEN_defer; ret string_copy_from_c(dst, "'defer'");
    case KAI_TOKEN_else; ret string_copy_from_c(dst, "'else'");
    case KAI_TOKEN_enum; ret string_copy_from_c(dst, "'enum'");
    case KAI_TOKEN_for; ret string_copy_from_c(dst, "'for'");
    case KAI_TOKEN_if; ret string_copy_from_c(dst, "'if'");
    case KAI_TOKEN_loop; ret string_copy_from_c(dst, "'loop'");
    case KAI_TOKEN_ret; ret string_copy_from_c(dst, "'ret'");
    case KAI_TOKEN_struct; ret string_copy_from_c(dst, "'struct'");
    case KAI_TOKEN_union; ret string_copy_from_c(dst, "'union'");
    case KAI_TOKEN_using; ret string_copy_from_c(dst, "'using'");
    case KAI_TOKEN_while; ret string_copy_from_c(dst, "'while'");
    case;
        dst.data[0] = #char "'";
        i: u32 = 0;
        while i <= 3 {
            ch: u8 = (id >> (i*8)) -> u8;
            if ch == 0 break;
            dst.data[i+1] = ch;
            i += 1;
        }
        dst.data[i+1] = #char "'";
        ret string.{count = i+2, data = dst.data};
    }
}

Tokenizer :: struct {
    current_token : Token;
    peeked_token  : Token;
    source        : string;
    cursor        : u32;
    line_number   : u32;
    peeking       : bool;
    string_arena  : Fixed_Allocator;
}

_parse_fractional_part :: (source: string, offset: *u32, start: Number) -> Number
{
    // Fractional Part
    if ([offset] < source.count && source.data[[offset]] == #char "."
    && !([offset]+1 < source.count && source.data[[offset]+1] == #char "."))
    {
        [offset] += 1;
        decimal: Number = number_parse_decimal(source, offset);
        start = number_add(start, decimal);
    }

    // Parse Exponential Part
    if ([offset] < source.count
    && (source.data[[offset]] == #char "e" || source.data[[offset]] == #char "E"))
    {
        is_neg: bool = false;
        
        [offset] += 1;
        if ([offset] < source.count)
        {
            if (source.data[[offset]] == #char "-") { [offset] += 1; is_neg = true; }
            if (source.data[[offset]] == #char "+") { [offset] += 1; } // skip
        }
        
        exponent: Number = number_parse_exponent(source, offset);
        if (is_neg) exponent = number_inv(exponent);
        start = number_mul(start, exponent);
    }

    ret start;
}

_make_multi_token :: (context: *Tokenizer, t: *Token, current: u8) -> bool
{
    if context.cursor >= context.source.count
        ret false;

    if current == {
    case #char "&"; {
        if context.source.data[context.cursor] == #char "&" {
            t.id = #multi "&&"; ret true;
        }
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "&="; ret true;
        }
    }
    case #char "|"; {
        if context.source.data[context.cursor] == #char "|" {
            t.id = #multi "||"; ret true;
        }
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "|="; ret true;
        }
    }
    case #char "="; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "=="; ret true;
        }
        if context.source.data[context.cursor] == #char ">" {
            t.id = #multi "=>"; ret true;
        }
    }
    case #char ">"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi ">="; ret true;
        }
        if context.source.data[context.cursor] == #char ">" {
            t.id = #multi ">>"; ret true;
        }
    }
    case #char "<"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "<="; ret true;
        }
        if context.source.data[context.cursor] == #char "<" {
            t.id = #multi "<<"; ret true;
        }
    }
    case #char "!"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "!="; ret true;
        }
    }
    case #char "+"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "+="; ret true;
        }
    }
    case #char "-"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "-="; ret true;
        }
        if context.source.data[context.cursor] == #char ">" {
            t.id = #multi "->"; ret true;
        }
        if (context.cursor + 1 < context.source.count &&
            context.source.data[context.cursor] == #char "-" &&
            context.source.data[context.cursor+1] == #char "-"
        ) {
            t.id = #multi "---";
            context.cursor += 1;
            t.string.count += 1;
            ret true;
        }
    }
    case #char "*"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "*="; ret true;
        }
    }
    case #char "/"; {
        if context.source.data[context.cursor] == #char "=" {
            t.id = #multi "/="; ret true;
        }
    }
    }
    ret false;
}

_W :: 1 << 1 | 1; // White space
_T :: 2 << 1 | 1; // Character Token
_D :: 3 << 1 | 1; // Directive
_G :: 4 << 1 | 1; // Tag
_K :: 1 << 1 | 0; // Keyword
_N :: 2 << 1 | 0; // Number
_C :: 5 << 1 | 1; // Comment
_S :: 7 << 1 | 1; // String
_Z :: 8 << 1 | 1; // Dot Symbol

// TODO: table isn't really necessary
_token_lookup_table: [128] u8 = .{
//  0   1   2   3   4   5   6   7   8   9   A   B   C   D   E   F
   _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, // 0
   _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, _W, // 1
   _W, _T, _S, _D, _T, _T, _T, _T, _T, _T, _T, _T, _T, _T, _Z, _C, // 2
   _N, _N, _N, _N, _N, _N, _N, _N, _N, _N, _T, _T, _T, _T, _T, _T, // 3
   _G,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, // 4
    0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, _T, _T, _T, _T,  0, // 5
   _T,  0, _K, _K, _K, _K, _K,  0,  0, _K,  0,  0, _K,  0,  0,  0, // 6
    0,  0, _K, _K,  0, _K,  0, _K,  0,  0,  0, _T, _T, _T, _T, _W, // 7
};

_tokenizer_advance_to_identifier_end :: (context: *Tokenizer)
{
    while (context.cursor < context.source.count) {
        c: u8 = context.source.data[context.cursor];
        if c < 128 && (_token_lookup_table[c]&1)
            break;
        context.cursor += 1;
    }
}

tokenizer_generate :: (context: *Tokenizer) -> Token
{
    token: Token = Token.{
        id = KAI_TOKEN_END,
        line_number = context.line_number,
    };

    while context.cursor < context.source.count
    {
        token.string.data = context.source.data + context.cursor;
        ch: u8 = token.string.data[0];
        where: u32;

        // treat every multi-byte unicode symbol as just an identifier
        if !(ch & 0x80)
            where = _token_lookup_table[ch];

        if where == {
        /////////////////////////////////////////////////////////////////////////////////
        // Whitespace
        case _W; {
            if ch == #char "\r" {
                // Handle Windows: CR LF
                if _next_character_equals(#char "\n")
                    context.cursor += 1;
                context.line_number += 1;
            }
            else if ch == #char "\n" {
                context.line_number += 1;
            }
            token.line_number = context.line_number;
            context.cursor += 1;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Numbers
        case _N; {
            token.id = KAI_TOKEN_NUMBER;
            start: u32 = context.cursor;

            // Integer
            if ch == #char "0" {
                context.cursor += 1;
                if context.cursor < context.source.count
                {
                    if (context.source.data[context.cursor] == #char "b")
                    {
                        context.cursor += 1;
                        token.value.number = number_parse_whole(context.source, *context.cursor, 2);
                        token.string.count = context.cursor - start;
                        ret token;
                    }
                    if (context.source.data[context.cursor] == #char "x")
                    {
                        context.cursor += 1;
                        token.value.number = number_parse_whole(context.source, *context.cursor, 16);
                        token.string.count = context.cursor - start;
                        ret token;
                    }
                }
            }
            token.value.number = number_parse_whole(context.source, *context.cursor, 10);
            token.value.number = _parse_fractional_part(context.source, *context.cursor, token.value.number);
            token.string.count = context.cursor - start;
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Identifiers
        case 0; {
            token.id = KAI_TOKEN_IDENTIFIER;
            start: u32 = context.cursor;
            context.cursor += 1;
            _tokenizer_advance_to_identifier_end(context);
            token.string.count = context.cursor - start;
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Directives
        case _D; {
            token.id = KAI_TOKEN_DIRECTIVE;
            start: u32 = context.cursor;
            context.cursor += 1;
            _tokenizer_advance_to_identifier_end(context);
            token.string.count = context.cursor - start;
            token.value.string = token.string;
            token.value.string.count -= 1;
            token.value.string.data += 1;
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Tags
        case _G; {
            token.id = KAI_TOKEN_TAG;
            start: u32 = context.cursor;
            context.cursor += 1;
            _tokenizer_advance_to_identifier_end(context);
            token.string.count = context.cursor - start;
            token.value.string = token.string;
            token.value.string.count -= 1;
            token.value.string.data += 1;
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Keywords
        case _K; {
            token.id = KAI_TOKEN_IDENTIFIER;
            start: u32 = context.cursor;
            context.cursor += 1;
            while (context.cursor < context.source.count) {
                c: u8 = context.source.data[context.cursor];
                if c < 128 && (_token_lookup_table[c]&1)
                    break;
                context.cursor += 1;
            }
            token.string.count = context.cursor - start;

            // Check if the string we just parsed is a keyword
            hash: u32 = _hash_keyword(token.string);
            keyword_index: u8 = _keyword_map[hash];
            if string_equals(_keywords[keyword_index], token.string) {
                token.id = 0x80 | keyword_index;
                ret token;
            }
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Strings
        case _S; {
            token.id = KAI_TOKEN_STRING;
            token.string.count = 1;
            context.cursor += 1;
            count: u32 = 0;
            token.value.string = string.{0};
            while (context.cursor < context.source.count) {
                if context.source.data[context.cursor] == #char "\"" {
                    break;
                }
                m: u8 = context.source.data[context.cursor];
                if m == #char "\\" {
                    context.cursor += 1;
                    token.string.count += 1;
                    if context.cursor >= context.source.count
                        break;
                    
                    m = context.source.data[context.cursor];
                    if m == {
                    case #char "\\"; m = #char "\\";
                    case #char "\""; m = #char "\"";
                    case #char "t"; m = #char "\t";
                    case #char "r"; m = #char "\r";
                    case #char "n"; m = #char "\n";
                    case #char "e"; m = #char "\e";
                    // TODO: output warning here somehow
                    //case;
                        //printf("!!! warning: could not escape '%c'\n", ch);
                    }
                }
                data: *u8 = cast fixed_allocate(*context.string_arena, 1);
                assert(data != null);
                [data] = m;
                if (token.value.string.data == null)
                    token.value.string.data = data;
                count += 1;
                context.cursor += 1;
            }
            context.cursor += 1;
            token.string.count += count + 1;
            token.value.string.count = count;
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Comment
        case _C; {
            token.string.data = context.source.data + context.cursor;
            context.cursor += 1;

            if context.cursor < context.source.count && context.source.data[context.cursor] == #char "/" {
                // single line comment
                while (context.cursor < context.source.count) {
                    if (context.source.data[context.cursor] == #char "\r" ||
                        context.source.data[context.cursor] == #char "\n")
                        break;
                    context.cursor += 1;
                }
                break;
            }

            // multi-line comment
            if (context.source.data[context.cursor] == #char "*") {
                context.cursor += 1;
                // TODO: need to skip "/*" and "*/" in strings
                depth: u32 = 1;
                while depth > 0 && context.cursor < context.source.count
                {
                    if (context.source.data[context.cursor] == #char "/"
                    &&  (context.cursor + 1) < context.source.count
                    &&  context.source.data[context.cursor+1] == #char "*") {
                        context.cursor += 2;
                        depth += 1;
                        continue;
                    }

                    if (context.source.data[context.cursor] == #char "*"
                    &&  (context.cursor + 1) < context.source.count
                    &&  context.source.data[context.cursor+1] == #char "/") {
                        context.cursor += 2;
                        depth -= 1;
                        continue;
                    }

                    if (context.source.data[context.cursor] == #char "\r") {
                        // Handle Windows: CR LF
                        if _next_character_equals(#char "\n")
                            context.cursor += 1;
                        context.line_number += 1;
                    }
                    else if (context.source.data[context.cursor] == #char "\n") {
                        context.line_number += 1;
                    }

                    context.cursor += 1;
                }
                break;
            }

            // otherwise a character token
            token.id = ch->u32;
            token.string.count = 1;
            if _make_multi_token(context, *token, ch) {
                context.cursor += 1;
                token.string.count += 1;
            }
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Single/Multi Character Tokens
        case _T; {
            token.id = ch->u32;
            token.string.count = 1;
            context.cursor += 1;
            if _make_multi_token(context, *token, ch) {
                context.cursor += 1;
                token.string.count += 1;
            }
            ret token;
        }
        /////////////////////////////////////////////////////////////////////////////////
        // Dot Token
        case _Z; {
            if _next_character_equals(#char ".") {
                context.cursor += 2;
                token.string.count = 2;
                token.id = #multi "..";
                ret token;
            }
            if (context.cursor + 1) < context.source.count &&
                context.source.data[context.cursor + 1] >= #char "0" &&
                context.source.data[context.cursor + 1] <= #char "9"
            {
                token.id = KAI_TOKEN_NUMBER;
                start: u32 = context.cursor;
                token.value.number = _parse_fractional_part(context.source, *context.cursor, token.value.number);
                token.string.count = context.cursor - start;
                ret token;
            }
            // Not ".." or Number? then single character token
            context.cursor += 1;
            token.string.count = 1;
            token.id = ch->u32;
            ret token;
        }
        }
    }
    ret token;
}

tokenizer_next :: (context: *Tokenizer) -> *Token
{
    if !context.peeking {
        context.current_token = tokenizer_generate(context);
        ret *context.current_token;
    }
    context.peeking = false;
    context.current_token = context.peeked_token;
    ret *context.current_token;
}

tokenizer_peek :: (context: *Tokenizer) -> *Token
{
    if context.peeking
        ret *context.peeked_token;
    context.peeking = true;
    context.peeked_token = tokenizer_generate(context);
    ret *context.peeked_token;
}

// Parser

Parser :: struct {
    tokenizer : Tokenizer;
    arena     : Arena_Allocator;
    error     : *Error;
}

_error_unexpected :: (parser: *Parser, token: *Token, where: string, wanted: string) -> *Expr
{
    // Report only the first Syntax Error
    if parser.error.result != KAI_SUCCESS
        ret null;

    buffer: Buffer = Buffer.{allocator = parser.arena.base};
    temp: [32] u8;
    temp_string: string = string.{count = sizeof(temp), data = temp};

    _buffer_append_string(*buffer, STRING("unexpected "));
    _buffer_append_string(*buffer, token_string(token.id, temp_string));
    _buffer_append_string(*buffer, STRING(" "));
    _buffer_append_string(*buffer, where);
    message: Range = _buffer_end(*buffer);
    memory: Memory = _buffer_done(*buffer);

    [parser.error] = Error.{
        result = KAI_ERROR_SYNTAX,
        location = Location.{
            string = token.string,
            line = token.line_number,
        },
        message = _range_to_string(message, memory),
        context = wanted,
        memory = memory,
    };

    ret null;
}

TOP_PRECEDENCE  :: 1;
PRECEDENCE_MASK :: 0x0FFFF;
_PREC_CAST      :: 0x00900;
_PREC_UNARY     :: 0x01000;

_Operator :: struct {
    prec : u32;
    type : u32;
}

_Operator_Type :: enum u32 {
    BINARY = 0; INDEX = 1; PROCEDURE_CALL = 2;
}

_operator_info :: (op: u32) -> _Operator
{
    if op == {
    case #multi "||"; ret _Operator.{0x0010, KAI__OPERATOR_TYPE_BINARY};
    case #multi "&&"; ret _Operator.{0x0011, KAI__OPERATOR_TYPE_BINARY};
    case #multi "<="; ret _Operator.{0x0040, KAI__OPERATOR_TYPE_BINARY};
    case #multi ">="; ret _Operator.{0x0040, KAI__OPERATOR_TYPE_BINARY};
    case #char  "<";  ret _Operator.{0x0040, KAI__OPERATOR_TYPE_BINARY};
    case #char  ">";  ret _Operator.{0x0040, KAI__OPERATOR_TYPE_BINARY};
    case #multi "=="; ret _Operator.{0x0050, KAI__OPERATOR_TYPE_BINARY};
    case #multi "!="; ret _Operator.{0x0050, KAI__OPERATOR_TYPE_BINARY};
    case #char  "|";  ret _Operator.{0x0070, KAI__OPERATOR_TYPE_BINARY};
    case #char  "^";  ret _Operator.{0x0070, KAI__OPERATOR_TYPE_BINARY};
    case #char  "&";  ret _Operator.{0x0070, KAI__OPERATOR_TYPE_BINARY};
    case #multi "<<"; ret _Operator.{0x0080, KAI__OPERATOR_TYPE_BINARY};
    case #multi ">>"; ret _Operator.{0x0080, KAI__OPERATOR_TYPE_BINARY};
    case #char  "+";  ret _Operator.{0x0100, KAI__OPERATOR_TYPE_BINARY};
    case #char  "-";  ret _Operator.{0x0100, KAI__OPERATOR_TYPE_BINARY};
    case #char  "*";  ret _Operator.{0x0200, KAI__OPERATOR_TYPE_BINARY};
    case #char  "/";  ret _Operator.{0x0200, KAI__OPERATOR_TYPE_BINARY};
    case #char  "%";  ret _Operator.{0x0200, KAI__OPERATOR_TYPE_BINARY};
    case #char  ".";  ret _Operator.{0xFFFF, KAI__OPERATOR_TYPE_BINARY};  // member access
    case #multi "->"; ret _Operator.{KAI__PREC_CAST, KAI__OPERATOR_TYPE_BINARY};
    case #char  "[";  ret _Operator.{0xBEEF, KAI__OPERATOR_TYPE_INDEX};
    case #char  "(";  ret _Operator.{0xBEEF, KAI__OPERATOR_TYPE_PROCEDURE_CALL};
    case;             ret _Operator.{0};
    }
}

_parser_create_identifier :: (parser: *Parser, token: Token) -> *Expr
{
    node: *Expr = cast arena_allocate(*parser.arena, sizeof(Expr));
    node.id = KAI_EXPR_IDENTIFIER;
    node.source_code = token.string;
    node.line_number = token.line_number;
    ret parse_tag_to_expr(parser, node -> *Expr);
}
_parser_create_string :: (parser: *Parser, token: Token) -> *Expr
{
    node: *Expr_String = cast arena_allocate(*parser.arena, sizeof(Expr_String));
    node.id = KAI_EXPR_STRING;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.value = token.value.string;
    ret node -> *Expr;
}
_parser_create_number :: (parser: *Parser, token: Token) -> *Expr
{
    node: *Expr_Number = cast arena_allocate(*parser.arena, sizeof(Expr_Number));
    node.id = KAI_EXPR_NUMBER;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.value = token.value.number;
    ret node -> *Expr;
}
_parser_create_literal :: (parser: *Parser, token: Token, head: *Expr, count: u32) -> *Expr
{
    node: *Expr_Literal = cast arena_allocate(*parser.arena, sizeof(Expr_Literal));
    node.id = KAI_EXPR_LITERAL;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.head = head;
    node.count = count;
    ret node -> *Expr;
}
_parser_create_unary :: (parser: *Parser, op_token: Token, expr: *Expr) -> *Expr
{
    node: *Expr_Unary = cast arena_allocate(*parser.arena, sizeof(Expr_Unary));
    node.id = KAI_EXPR_UNARY;
    node.source_code = merge_strings(op_token.string, expr.source_code);
    node.line_number = _min_u32(op_token.line_number, expr.line_number);
    node.op = op_token.id;
    node.expr = expr;
    ret node -> *Expr;
}
_parser_create_binary :: (parser: *Parser, left: *Expr, right: *Expr, op: u32) -> *Expr
{
    node: *Expr_Binary = cast arena_allocate(*parser.arena, sizeof(Expr_Binary));
    node.id = KAI_EXPR_BINARY;
    node.source_code = merge_strings(left.source_code, right.source_code);
    node.line_number = _min_u32(left.line_number, right.line_number);
    node.op = op;
    node.left = left;
    node.right = right;
    ret node -> *Expr;
}
_parser_create_array :: (parser: *Parser, op_token: Token, expr: *Expr, rows: *Expr, cols: *Expr, flags: u8) -> *Expr
{
    node: *Expr_Array = cast arena_allocate(*parser.arena, sizeof(Expr_Array));
    node.id = KAI_EXPR_ARRAY;
    node.source_code = merge_strings(op_token.string, expr.source_code);
    node.line_number = _min_u32(op_token.line_number, expr.line_number);
    node.flags = flags;
    node.rows = rows;
    node.cols = cols;
    node.expr = expr;
    ret node -> *Expr;
}
_parser_create_special :: (parser: *Parser, token: Token, kind: u8) -> *Expr
{
    node: *Expr_Special = cast arena_allocate(*parser.arena, sizeof(Expr_Special));
    node.id = KAI_EXPR_SPECIAL;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.kind = kind;
    ret node -> *Expr;
}
_parser_create_procedure_type :: (parser: *Parser, in_out: *Expr, in_count: u8, out_count: u8) -> *Expr
{
    node: *Expr_Procedure_Type = cast arena_allocate(*parser.arena, sizeof(Expr_Procedure_Type));
    node.id = KAI_EXPR_PROCEDURE_TYPE;
    node.source_code = in_out.source_code;
    node.line_number = in_out.line_number;
    node.in_out_expr = in_out;
    node.in_count = in_count;
    node.out_count = out_count;
    ret node -> *Expr;
}
_parser_create_procedure_call :: (parser: *Parser, proc: *Expr, args: *Expr, arg_count: u8) -> *Expr
{
    node: *Expr_Procedure_Call = cast arena_allocate(*parser.arena, sizeof(Expr_Procedure_Call));
    node.id = KAI_EXPR_PROCEDURE_CALL;
    node.source_code = proc.source_code;
    node.line_number = proc.line_number;
    node.proc = proc;
    node.arg_head = args;
    node.arg_count = arg_count;
    ret node -> *Expr;
}
_parser_create_procedure :: (parser: *Parser, token: Token, in_out: *Expr, body: *Stmt, in_count: u8, out_count: u8) -> *Expr
{
    node: *Expr_Procedure = cast arena_allocate(*parser.arena, sizeof(Expr_Procedure));
    node.id = KAI_EXPR_PROCEDURE;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.in_out_expr = in_out;
    node.in_count = in_count;
    node.out_count = out_count;
    node.body = body;
    ret node -> *Expr;
}
_parser_create_import :: (parser: *Parser, token: Token, import: Token) -> *Expr
{
    node: *Expr = cast arena_allocate(*parser.arena, sizeof(Expr));
    node.id = KAI_EXPR_IMPORT;
    node.source_code = merge_strings(token.string, import.string);
    node.line_number = token.line_number;
    node.name = import.value.string;
    ret node -> *Expr;
}
_parser_create_struct :: (parser: *Parser, token: Token, field_count: u32, body: *Stmt) -> *Expr
{
    node: *Expr_Struct = cast arena_allocate(*parser.arena, sizeof(Expr_Struct));
    node.id = KAI_EXPR_STRUCT;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.field_count = field_count;
    node.head = body;
    ret node -> *Expr;
}
_parser_create_enum :: (parser: *Parser, token: Token, type: *Expr, field_count: u32, body: *Stmt) -> *Expr
{
    node: *Expr_Enum = cast arena_allocate(*parser.arena, sizeof(Expr_Enum));
    node.id = KAI_EXPR_ENUM;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.type = type;
    node.field_count = field_count;
    node.head = body;
    ret node -> *Expr;
}
_parser_create_return :: (parser: *Parser, ret_token: Token, expr: *Expr) -> *Expr
{
    node: *Stmt_Return = cast arena_allocate(*parser.arena, sizeof(Stmt_Return));
    node.id = KAI_STMT_RETURN;
    node.source_code = ret_token.string;
    node.line_number = ret_token.line_number;
    node.expr = expr;
    ret node -> *Expr;
}
_parser_create_declaration :: (parser: *Parser, name: string, type: *Expr, value: *Expr, flags: u8, line_number: u32) -> *Expr
{
    node: *Stmt_Declaration = cast arena_allocate(*parser.arena, sizeof(Stmt_Declaration));
    node.id = KAI_STMT_DECLARATION;
    node.source_code = name;
    node.line_number = line_number;
    node.name = name;
    node.type = type;
    node.value = value;
    node.flags = flags;
    ret parse_tag_to_expr(parser, node -> *Expr);
}
_parser_create_assignment :: (parser: *Parser, op: u32, dest: *Expr, value: *Expr) -> *Expr
{
    node: *Stmt_Assignment = cast arena_allocate(*parser.arena, sizeof(Stmt_Assignment));
    node.id = KAI_STMT_ASSIGNMENT;
    node.source_code = dest.source_code;
    node.line_number = dest.line_number;
    node.op = op;
    node.dest = dest;
    node.value = value;
    ret node -> *Expr;
}
_parser_create_if :: (parser: *Parser, if_token: Token, flags: u8, expr: *Expr, then_body: *Stmt, else_body: *Stmt) -> *Expr
{
    node: *Stmt_If = cast arena_allocate(*parser.arena, sizeof(Stmt_If));
    node.id = KAI_STMT_IF;
    node.source_code = if_token.string;
    node.line_number = if_token.line_number;
    node.flags = flags;
    node.condition = expr;
    node.then_body = then_body;
    node.else_body = else_body;
    ret node -> *Expr;
}
_parser_create_while :: (parser: *Parser, while_token: Token, expr: *Expr, body: *Stmt) -> *Expr
{
    node: *Stmt_While = cast arena_allocate(*parser.arena, sizeof(Stmt_While));
    node.id = KAI_STMT_WHILE;
    node.source_code = while_token.string;
    node.line_number = while_token.line_number;
    node.body = body;
    node.condition = expr;
    ret node -> *Expr;
}
_parser_create_for :: (parser: *Parser, for_token: Token, name: string, from: *Expr, to: *Expr, body: *Stmt, flags: u8) -> *Expr
{
    node: *Stmt_For = cast arena_allocate(*parser.arena, sizeof(Stmt_For));
    node.id = KAI_STMT_FOR;
    node.source_code = for_token.string;
    node.line_number = for_token.line_number;
    node.body = body;
    node.from = from;
    node.to = to;
    node.iterator_name = name;
    node.flags = flags;
    ret node -> *Expr;
}
_parser_create_control :: (parser: *Parser, token: Token, kind: u8, expr: *Expr) -> *Expr
{
    node: *Stmt_Control = cast arena_allocate(*parser.arena, sizeof(Stmt_Control));
    node.id = KAI_STMT_CONTROL;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.kind = kind;
    node.expr = expr;
    ret node -> *Expr;
}
_parser_create_compound :: (parser: *Parser, token: Token, body: *Stmt) -> *Expr
{
    node: *Stmt_Compound = cast arena_allocate(*parser.arena, sizeof(Stmt_Compound));
    node.id = KAI_STMT_COMPOUND;
    node.source_code = token.string;
    node.line_number = token.line_number;
    node.head = body;
    ret node -> *Expr;
}
_parser_create_tag :: (parser: *Parser, token: Token, expr: *Expr) -> *Tag
{
    tag: *Tag = cast arena_allocate(*parser.arena, sizeof(Tag));
    tag.name = token.value.string;
    tag.expr = expr;
    ret tag;
}

parse_procedure_call_arguments :: (parser: *Parser, arg_count: *u32) -> *Expr
{
    current: *Token = *parser.tokenizer.current_token;
    args: Expr_List;

    while current.id != #char ")"
    {
        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in argument list", "an expression");
        _linked_list_append(args, expr);
        _expect([arg_count] < 255, "in argument list", "too many arguments");
        [arg_count] += 1;
        _next_token(); // skip expr
        if current.id == #char ","
            _next_token(); // skip ','
        else if current.id != #char ")"
            ret _unexpected("in argument list", "',' or ')' expected here");
    }

    ret args.head;
}

parse_tag_to_expr :: (parser: *Parser, expr: *Expr) -> *Expr
{
    peeked: *Token = _peek_token();
    if peeked.id == KAI_TOKEN_TAG {
        token: Token = [peeked];
        _next_token(); // advance to tag

        tag_expr: *Expr;
        _peek_token();
        if peeked.id == #char "(" {
            _next_token(); // advance to '('
            _next_token(); // skip '('
            arg_count: u32;
            tag_expr = parse_procedure_call_arguments(parser, *arg_count);
        }

        expr.tag = _parser_create_tag(parser, token, tag_expr);
    }
    ret expr;
}

_is_procedure_next :: (parser: *Parser) -> bool
{
    peeked: *Token = _peek_token();
    if peeked.id == #char ")" ret true;

    state: Tokenizer = parser.tokenizer;
    current: *Token = _next_token();
    found: bool = false;

    // go until we hit ')' or END, searching for ':'
    while current.id != KAI_TOKEN_END && current.id != #char ")"
    {
        if current.id == #char ":" {
            found = true;
            break;
        }
        _next_token();
    }
    parser.tokenizer = state;
    ret found;
}

parse_expression :: (parser: *Parser, flags: u32) -> *Expr
{
    left: *Expr;
    current: *Token = *parser.tokenizer.current_token;

    if current.id == {
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Single-Token Expressions
    case KAI_TOKEN_IDENTIFIER; left = _parser_create_identifier(parser, [current]);
    case KAI_TOKEN_NUMBER;     left = _parser_create_number(parser, [current]);
    case KAI_TOKEN_STRING;     left = _parser_create_string(parser, [current]);
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Parenthesis
    case #char "("; {
        tokenizer_next(*parser.tokenizer);
        left = parse_expression(parser, TOP_PRECEDENCE);
        _expect(left != null, "in expression", "should be an expression here");
        tokenizer_next(*parser.tokenizer);
        _expect(current.id == #char ")", "in expression", "should be an operator or ')' here");
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Dereference Operator (or Array Type)
    case #char "["; {
        // Check if actually a type
        peeked: *Token = _peek_token();
        if peeked.id == #char "]" || peeked.id == #multi ".."
        || peeked.id == KAI_TOKEN_NUMBER { // Numbers are not pointers
            left = parse_type_expression(parser);
        }
        else {
            op_token: Token = [current];
            _next_token(); // skip op token
            left = parse_expression(parser, TOP_PRECEDENCE);
            _expect(left, "in dereference expression", "should be an expression here");
            _next_token(); // skip op token
            _expect(current.id == #char "]", "in dereference expression", "should be an operator or ']' here");
            left = _parser_create_unary(parser, op_token, left);
        }
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Literal Types
    case #char "{"; {
        token: Token = [current];
        body: Expr_List;
        count: u32;
        _next_token(); // skip '{'
        while current.id != #char "}"
        {
            token: Token = [current];
            name: string;
            peeked: *Token = _peek_token();
            if token.id == KAI_TOKEN_IDENTIFIER && peeked.id == #char "=" {
                name = token.string;
                _next_token(); // skip IDENTIFIER
                _next_token(); // skip '='
            }

            expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
            _expect(expr != null, "in literal expression", "should be an expression here");
            expr.name = name;

            _next_token();
            if current.id == #char "," _next_token();
            else _expect(current.id == #char "}", "in literal expression", "should be '}' or ',' here");

            _linked_list_append(body, expr);
            count += 1;
        }
        _expect(current.id == #char "}", "in literal expression", "should be a '}' here");
        ret _parser_create_literal(parser, token, body.head, count);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Unary Operators
    case #char "."; #through;
    case #char "~"; #through;
    case #char "!"; #through;
    case #char "-"; #through;
    case #char "+"; #through;
    case #char "*"; {
        op_token: Token = [current];
        _next_token(); // skip op token
        left = parse_expression(parser, _PREC_UNARY); // TODO: should unary have all same precidence?
        _expect(left, "in unary expression", "should be an expression here");
        left = _parser_create_unary(parser, op_token, left);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Explicit Casting "cast(int) x"
    case KAI_TOKEN_cast; {
        token: Token = [current];
        token.id = #multi "->";
        _next_token();
        type: *Expr;
        if current.id == #char "(" {
            _expect(current.id == #char "(", "in expression", "'(' after cast keyword");
            _next_token();
            type = parse_type_expression(parser);
            _expect(type, "in expression", "type");
            _next_token();
            _expect(current.id == #char ")", "in expression", "')' after Type in cast expression");
            _next_token();
        }
        expr: *Expr = parse_expression(parser, KAI__PREC_CAST);
        _expect(expr, "in expression", "should be expression or '(' here");
        if type == null
            left = _parser_create_unary(parser, token, expr);
        else 
            left = _parser_create_binary(parser, expr, type, token.id);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Directives
    case KAI_TOKEN_DIRECTIVE; {
        // TODO: directive map
        if (string_equals(current.value.string, STRING("size")))
        {
            left = _parser_create_special(parser, [current], KAI_SPECIAL_EVAL_SIZE);
        }
        else if (string_equals(current.value.string, STRING("type")))
        {
            left = _parser_create_special(parser, [current], KAI_SPECIAL_EVAL_TYPE);
        }
        else if (string_equals(current.value.string, STRING("Type")))
        {
            ret _parser_create_special(parser, [current], KAI_SPECIAL_TYPE);
        }
        else if (string_equals(current.value.string, STRING("Number")))
        {
            ret _parser_create_special(parser, [current], KAI_SPECIAL_NUMBER);
        }
        else if (string_equals(current.value.string, STRING("Code")))
        {
            ret _parser_create_special(parser, [current], KAI_SPECIAL_CODE);
        }
        else if (string_equals(current.value.string, STRING("import")))
        {
            token: Token = [current];
            _next_token();
            _expect(current.id == KAI_TOKEN_STRING, "in import", "string");
            left = _parser_create_import(parser, token, [current]);
        }
        else if (kai_string_equals(current.value.string, KAI_STRING("through")))
        {
            left = _parser_create_control(parser, [current], KAI_CONTROL_THROUGH, null);
        }
        else if (string_equals(current.value.string, STRING("char")))
        {
            _next_token();
            _expect(current.id == KAI_TOKEN_STRING, "char", "must be string");
            cp: u32;
            if current.value.string.count > _utf8_decode(current.value.string, *cp) {
                ret _error_unexpected(parser, current, STRING("in character literal"), STRING("string must be a single codepoint"));
            }
            current.value.number = number_normalize(Number.{n = cp->u64, d = 1});
            left = _parser_create_number(parser, [current]);
        }
        else if (string_equals(current.value.string, STRING("multi")))
        {
            _next_token();
            _expect(current.id == KAI_TOKEN_STRING, "multi", "must be string");
            _expect(current.value.string.count > 0, "multi", "string cannot be empty");
            value: Number = Number.{0};
            base: Number = Number.{n = 1, d = 1, e = 8}; 
            for i: 0..<current.value.string.count {
                idx: u32 = current.value.string.count - 1 - i;
                dg: Number = number_normalize(Number.{n = current.value.string.data[idx], d = 1});
                value = number_add(number_mul(value, base), dg);
            }
            current.value.number = value;
            left = _parser_create_number(parser, [current]);
        }
        else if (
            kai_string_equals(current.value.string, KAI_STRING("array"))
        ||	kai_string_equals(current.value.string, KAI_STRING("map"))
        ||	kai_string_equals(current.value.string, KAI_STRING("proc"))
        )
        {
            // TODO: make distinctions between these directives
            _next_token(); // skip directive
            left = parse_type_expression(parser);
        }
        else if (string_equals(current.value.string, STRING("Julie")))
        {
            current.string = STRING("\"<3\"");
            current.value.string = STRING("<3");
            left = _parser_create_string(parser, [current]);
        }
        else ret _unexpected("in expression", "unknown directive"); // TODO: custom error message
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Structs & Unions
    case KAI_TOKEN_union; #through;
    case KAI_TOKEN_struct; {
        struct_token: Token = [current];
        body: Stmt_List;
        count: u32;

        _next_token(); // skip struct
        _expect(current.id == #char "{", "in struct", "should be '{' here");
        _next_token(); // skip '{'
        while current.id != #char "}"
        {
            stmt: *Stmt = parse_statement(parser);
            _expect(stmt, "in struct definition", "expected a statement here");
            _linked_list_append(body, stmt);
            _next_token();
            count += 1;
        }

        left = _parser_create_struct(parser, struct_token, count, body.head);
        if struct_token.id == KAI_TOKEN_union
            left.flags |= KAI_FLAG_STRUCT_UNION;
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Enum
    case KAI_TOKEN_enum; {
        struct_token: Token = [current];
        body: Stmt_List;
        count: u32;

        _next_token(); // skip enum

        type: *Expr = parse_type_expression(parser);
        // TODO: need some way to replace error message here,
        // otherwise this error message will be terrible
        _expect(type, "[todo]", "[todo]");
        _next_token();

        _next_token(); // skip '{'
        while current.id != #char "}"
        {
            stmt: *Stmt = parse_statement(parser);
            _expect(stmt, "in struct definition", "expected a statement here");
            _linked_list_append(body, stmt);
            _next_token();
            count += 1;
        }

        left = _parser_create_enum(parser, struct_token, type, count, body.head);
    }
    case; ret null;
    }

    peeked: *Token = tokenizer_peek(*parser.tokenizer);
    op_info: _Operator = _operator_info(peeked.id);
    prec: u32 = flags & PRECEDENCE_MASK;

    // handle precedence by returning early
    while op_info.prec != 0 && op_info.prec > prec
    {
        op: u32 = peeked.id;
        
        if op == #multi "==" {
            prev_state: Tokenizer = parser.tokenizer;
            tokenizer_next(*parser.tokenizer);
            tokenizer_next(*parser.tokenizer);
            if (current.id == #char "{") {
                parser.tokenizer = prev_state;
                ret left; // if-case
            }
        }
        else {
            tokenizer_next(*parser.tokenizer);
            tokenizer_next(*parser.tokenizer);
        }

        if op_info.type == {
        case KAI__OPERATOR_TYPE_BINARY; {
            right: *Expr = parse_expression(parser, op_info.prec);
            _expect(right != null, "in binary expression", "should be an expression after binary operator");
            left = _parser_create_binary(parser, left, right, op);
        }
        case KAI__OPERATOR_TYPE_INDEX; {
            right: *Expr = parse_expression(parser, TOP_PRECEDENCE);
            _expect(right, "in index operation", "should be an expression here");
            _next_token();
            _expect(current.id == #char "]", "in index operation", "expected ']' here");
            left = _parser_create_binary(parser, left, right, op);
        }
        case KAI__OPERATOR_TYPE_PROCEDURE_CALL; {
            arg_count: u32;
            args: *Expr = parse_procedure_call_arguments(parser, *arg_count);
            left = _parser_create_procedure_call(parser, left, args, arg_count -> u8);
        }
        }

        tokenizer_peek(*parser.tokenizer);
        op_info = _operator_info(peeked.id);
    }

    ret left;
}

parse_type_expression :: (parser: *Parser) -> *Expr
{
    current: *Token = *parser.tokenizer.current_token;

    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Array/Slice Types
    if current.id == {
    case #char "["; {
        op_token: Token = [current];
        _next_token(); // skip '['

        rows: *Expr;
        cols: *Expr;
        expr_flags: u8;

        if current.id == #multi ".." {
            _next_token();
            expr_flags |= KAI_FLAG_ARRAY_DYNAMIC;
        }
        else if current.id != #multi "]" {
            rows = parse_expression(parser, TOP_PRECEDENCE);
            _expect(rows, "in array type", "[todo]");
        
            _next_token(); // skip expr
            if current.id == #char "," {
                _next_token(); // skip ','
                cols = parse_expression(parser, TOP_PRECEDENCE);
                _expect(cols, "in array type", "[todo]");
                _next_token(); // skip expr
            }
        }

        _expect(current.id == #char "]", "in array type", "array");
        _next_token(); // skip ']'
        type: *Expr = parse_type_expression(parser);
        _expect(type, "in array type", "expected type here");
        // Types do not have binary operators, so return early
        ret _parser_create_array(parser, op_token, type, rows, cols, expr_flags);
    }
    case #char "*"; {
        op_token: Token = [current];
        _next_token(); // skip op token
        expr: *Expr = parse_type_expression(parser); // TODO: should unary have all same precidence?
        _expect(expr, "in unary expression", "should be an expression here");
        ret _parser_create_unary(parser, op_token, expr);
    }
    }
    
    if current.id != #char "("
        ret parse_expression(parser, TOP_PRECEDENCE);

    _next_token(); // skip '('

    in_count: u8;
    out_count: u8;
    in_out: Expr_List;

    // Parse Procedure input types
    while current.id != #char ")"
    {
        name: string;
        if current.id == KAI_TOKEN_IDENTIFIER {
            peeked: *Token = _peek_token();			
            if peeked.id == #char ":" {
                name = current.string;
                _next_token(); // get ':'
                _next_token(); // get what is after
            }
        }

        type: *Expr = parse_type_expression(parser);
        _expect(type, "in procedure type", "should be a type here");

        type.name = name;
        _linked_list_append(in_out, type);

        _expect(in_count != 255, "in procedure type", "too many inputs to procedure");
        in_count += 1;

        _next_token();  // get ',' token or ')'

        if current.id == #char ","
            _next_token();
        else if current.id != #char ")"
            ret _unexpected("in procedure type", "',' or ')' expected here");
    }

    peek: *Token = _peek_token();  // see if we have any returns

    if peek.id == #multi "->" {
        _next_token(); // eat '->'
        _next_token(); // get token after

        enclosed_return: bool = false;
        if current.id == #char "(" {
            enclosed_return = true;
            _next_token();
        }

        while current.id != #char ")"
        {
            type: *Expr = parse_type_expression(parser);
            if !type ret null;

            _linked_list_append(in_out, type);
            _expect(out_count != 255, "in procedure type", "too many inputs to procedure");
            out_count += 1;

            _peek_token();  // get ',' token or something else
            if peek.id == #char "," {
                _next_token();  // get ','
                _next_token();  // skip ','
            }
            else break;
        }

        if enclosed_return {
            if peek.id != #char ")" {
                _next_token();
                ret _unexpected("in procedure type", "should be ')' after return types");
            }
            else _next_token();
        }
    }
    ret _parser_create_procedure_type(parser, in_out.head, in_count, out_count);
}

parse_procedure :: (parser: *Parser) -> *Expr
{
    current: *Token = *parser.tokenizer.current_token;
    token: Token = [current];

    // sanity check
    _expect(current.id == #char "(", "", "this is likely a compiler bug, sorry :c");
    _next_token(); // skip '('

    in_count: u8;
    out_count: u8;
    in_out: Expr_List;

    while current.id != #char ")"
    {
        flags: u8;
        if current.id == KAI_TOKEN_using {
            flags |= KAI_FLAG_PROC_USING;
            _next_token();
        }
        _expect(current.id == KAI_TOKEN_IDENTIFIER, "in procedure input", "should be an identifier");
        name: string = current.string;
        _next_token();
        _expect(current.id == #char ":", "in procedure input", "wanted a ':' here");
        _next_token();
        type: *Expr = parse_type_expression(parser);
        _expect(type, "in procedure input", "should be type");

        type.name = name;
        type.flags = flags;

        _linked_list_append(in_out, type);
        _expect(in_count != 255, "in procedure call", "too many inputs to procedure");
        in_count += 1;

        _next_token();
        if current.id == {
        case #char ")";
        case #char ","; _next_token();
        case; ret _unexpected("in procedure input", "wanted ')' or ',' here");
        }
    }
    _next_token();

    // return value
    if current.id == #multi "->" {
        _next_token();
        type: *Expr = parse_type_expression(parser);
        _expect(type, "in procedure return type", "should be type");

        _linked_list_append(in_out, type);
        _expect(out_count != 255, "in procedure call", "too many inputs to procedure");
        out_count += 1;

        _next_token();
    }

    body: *Stmt;
    if current.id == KAI_TOKEN_DIRECTIVE && string_equals(STRING("host"), current.string) {
        _next_token();
        _expect(current.id == #char ";", "???", "???");
    }
    else {
        body = parse_statement(parser);
        if !body ret _unexpected("[todo: remove this]", "");
    }

    ret _parser_create_procedure(parser, token, in_out.head, body, in_count, out_count);
}

parse_declaration :: (parser: *Parser) -> *Stmt
{
    current: *Token = *parser.tokenizer.current_token;

    if current.id == KAI_TOKEN_DIRECTIVE
        ret parse_statement(parser);

    if current.id != KAI_TOKEN_IDENTIFIER
        ret _unexpected("in declaration", "expected an identifier");

    name: string = current.string;
    line_number: u32 = current.line_number;

    _next_token(); // skip identifier

    if current.id != #char ":"
        ret _unexpected("in declaration", "expected ':' here");

    _next_token(); // skip ':'

    flags: u8 = 0;
    type: *Expr = parse_type_expression(parser);

    if type _next_token(); // skip type if there was one

    expr: *Expr;

    if current.id == {
    case #char ":"; flags |= KAI_FLAG_DECL_CONST;
    case #char "=";
    case #char ";";
        _expect(type != null, "in declaration", "should be '=', ':', or expression here");
        ret _parser_create_declaration(parser, name, type, expr, flags, line_number);
    case;
        ret _unexpected("in declaration", "should be '=', ':', or ';'");
    }
    _next_token(); // skip '=', ':', or ';'

    // enums, structs, procedures do not require semicolon
    require_semicolon: bool = false;

    if current.id == KAI_TOKEN_DIRECTIVE && string_equals(current.value.string, STRING("host_import")) {
        flags |= KAI_FLAG_DECL_HOST_IMPORT;
        require_semicolon = true;
    }
    else if current.id == #char "(" && _is_procedure_next(parser) {
        expr = parse_procedure(parser);
        if !expr ret _unexpected("[todo]", "[todo]");
    }
    else {
        require_semicolon = true;
        if (current.id == KAI_TOKEN_struct || current.id == KAI_TOKEN_union || current.id == KAI_TOKEN_enum)
            require_semicolon = false;
        expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in declaration", "should be an expression here");
    }
    
    if require_semicolon {
        _next_token();
        _expect(current.id == #char ";", "after declaration", "there should be a ';' before this");
    }
    else {
        peeked: *Token = _peek_token();
        if peeked.id == #char ";" _next_token(); // go to semicolon
    }

    ret _parser_create_declaration(parser, name, type, expr, flags, line_number);
}

parse_statement :: (parser: *Parser) -> *Stmt
{
    current: *Token = *parser.tokenizer.current_token;
    if current.id == {
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Compound Statements
    case #char "{"; {
        token: Token = [current];
        _next_token();
        body: Stmt_List;

        while current.id != #char "}" {
            statement: *Stmt = parse_statement(parser);
            if !statement ret null;
            _linked_list_append(body, statement);
            _next_token();  // eat ';' (get token after)
        }

        // TODO: is this OK?
        // No need for compound if there is only one statement
        // if (statement_array.count == 1) {
        //    p_tarray_destroy(statement_array);
        //    return *(Kai_Stmt*)((Kai_u8*)parser->memory.temperary +
        //    statement_array.offset);
        //}

        ret _parser_create_compound(parser, token, body.head);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Control Statements
    case KAI_TOKEN_case; #through;
    case KAI_TOKEN_break; #through;
    case KAI_TOKEN_continue; #through;
    case KAI_TOKEN_defer; {
        token: Token = [current];
        _next_token(); // skip token

        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        if expr != null _next_token();
        _expect(current.id == #char ";", "after control statement", "there should be a ';' before this");
        kind: u8 = KAI_CONTROL_CASE;
        if token.id == KAI_TOKEN_break    kind = KAI_CONTROL_BREAK;
        if token.id == KAI_TOKEN_continue kind = KAI_CONTROL_CONTINUE;
        if token.id == KAI_TOKEN_defer    kind = KAI_CONTROL_DEFER;
        ret _parser_create_control(parser, token, kind, expr);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Using Statements
    case KAI_TOKEN_using; {
        _next_token(); // skip 'using'
        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in using statement", "should be an expression");
        _next_token();
        _expect(current.id == #char ";", "after statement", "there should be a ';' before this");
        expr.flags = KAI_FLAG_EXPR_USING;
        ret expr;
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Return Statements
    case KAI_TOKEN_ret; {
        ret_token: Token = [current];
        _next_token(); // skip 'ret'

        if current.id == #char ";"
            ret _parser_create_return(parser, ret_token, null);

        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in return statement", "should be an expression");
        _next_token();
        _expect(current.id == #char ";", "after statement", "there should be a ';' before this");

        ret _parser_create_return(parser, ret_token, expr);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // If Statements
    case KAI_TOKEN_if; {
        if_token: Token = [current];
        _next_token(); // skip 'if'
        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in if statement", "should be an expression here");
        _next_token();
        flags: u8 = 0;
        if current.id == #multi "==" {
            _next_token();
            flags |= KAI_FLAG_IF_CASE;
        }
        then_body: *Stmt = parse_statement(parser);
        if !then_body ret null;
        peeked: *Token = _peek_token();
        else_body: *Stmt;
        if peeked.id == KAI_TOKEN_else {
            _next_token();
            _next_token();
            else_body = parse_statement(parser);
            if !else_body ret null;
        }
        ret _parser_create_if(parser, if_token, flags, expr, then_body, else_body);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // While Statements
    case KAI_TOKEN_while; {
        while_token: Token = [current];
        _next_token(); // skip 'while'
        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _next_token();
        body: *Stmt = parse_statement(parser);
        ret _parser_create_while(parser, while_token, expr, body);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // For Statements
    case KAI_TOKEN_for; {
        for_token: Token = [current];
        _next_token(); // skip 'for'
        _expect(current.id == KAI_TOKEN_IDENTIFIER, "in for statement", "should be the name of the iterator");
        iterator_name: string = current.string;
        _next_token();
        _expect(current.id == #char ":", "in for statement", "should be ':' here");
        _next_token();
        from: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(from, "in for statement", "should be an expression here");
        to: *Expr;
        _next_token();
        flags: u8;
        if current.id == #multi ".." {
            _next_token();
            if current.id == #char "<" {
                flags |= KAI_FLAG_FOR_LESS_THAN;
                _next_token(); // skip '<'
            }
            to = parse_expression(parser, TOP_PRECEDENCE);
            _expect(to, "in for statement", "should be an expression here");
            _next_token();
        }
        body: *Stmt = parse_statement(parser);
        if !body ret null;
        ret _parser_create_for(parser, for_token, iterator_name, from, to, body, flags);
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Statements with Directives
    case KAI_TOKEN_DIRECTIVE; {
        if (string_equals(current.value.string, STRING("export")))
        {
            _next_token();
            decl: *Stmt = parse_declaration(parser);
            _expect(decl, "in export statement", "should be declaration here");
            decl.flags |= KAI_FLAG_DECL_EXPORT;
            ret decl;
        }
        else
        {
            expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
            _expect(expr, "in statement", "there should be an expression here");
            _next_token();
            _expect(current.id == #char ";", "after statement", "there should be a ';' before this");
            ret expr;
        }
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Handle Possible Declarations
    case KAI_TOKEN_IDENTIFIER; {
        peeked: *Token = _peek_token();
        if peeked.id == #char ":" ret parse_declaration(parser);
    } #through;
    ///////////////////////////////////////////////////////////////////////////////////////////////
    // Expression/Assignment Statements
    case; {
        requires_semicolon: bool = true; // TODO: should ever not require?
        expr: *Expr = parse_expression(parser, TOP_PRECEDENCE);
        _expect(expr, "in statement", "should be an expression or statement");
        peeked: *Token = _peek_token();

        if peeked.id == {
        case #char "="; #through;
        case #multi "|="; #through;
        case #multi "&="; #through;
        case #multi "+="; #through;
        case #multi "-="; #through;
        case #multi "*="; #through;
        case #multi "/="; {
            _next_token(); // skip expr
            op: u32 = current.id;
            _next_token();
            right: *Expr = parse_expression(parser, TOP_PRECEDENCE);
            _expect(right, "in assignment statement", "should be an expression");
            expr = _parser_create_assignment(parser, op, expr, right);
            _next_token();
            _expect(current.id == #char ";", "in assignment statement", "should be ';' before this");
        }
        case; {
            if requires_semicolon {
                _next_token(); // skip expr
                _expect(current.id == #char ";", "in expression statement", "should be ';' before this");
            }
        }
        }
        ret expr;
    }
    }
}

create_syntax_tree :: (info: *Syntax_Tree_Create_Info, out_tree: *Syntax_Tree) -> Result
{
    parser: Parser;
    parser.tokenizer.source = info.source.contents;
    parser.tokenizer.line_number = 1;
    parser.error = info.error;
    parser.tokenizer.string_arena = Fixed_Allocator.{
        data = info.allocator.heap_allocate(info.allocator.user, null, info.source.contents.count, 0),
        size = info.source.contents.count,
    };
    arena_create(*parser.arena, *info.allocator);

    statements: Stmt_List;
    token: *Token = tokenizer_next(*parser.tokenizer);
    while token.id != KAI_TOKEN_END {
        statement: *Stmt = parse_declaration(*parser);
        if statement == null break;
        _linked_list_append(statements, statement);
        tokenizer_next(*parser.tokenizer);
    }

    out_tree.root.id = KAI_STMT_COMPOUND;
    out_tree.root.head = statements.head;
    out_tree.source = info.source;
    out_tree.allocator = parser.arena;

    if parser.error.result != KAI_SUCCESS
        parser.error.location.source = info.source;
    ret parser.error.result;
}

destroy_syntax_tree :: (tree: *Syntax_Tree)
{
    tree->void;
    // TODO: free sometime?
}
